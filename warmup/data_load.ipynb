{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "from datetime import date, timedelta\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Country Level Data ## \n",
    "Here we will use the Hopkins API for Quering its data - it is available [here](https://coviddata.github.io/covid-api/). We will pull the country level data.\n",
    "- ` country_confirmed_df` - Country level time series data on the number of confirmed cases \n",
    "- ` country_deaths_df` - Country level time series data on the number of deaths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_confirmed_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "country_deaths_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n",
    "country_confirmed_df['Country/Region'].replace(['US'], 'United States',inplace=True)\n",
    "country_deaths_df['Country/Region'].replace(['US'], 'United States',inplace=True)\n",
    "country_deaths_df = pd.DataFrame(country_deaths_df.groupby('Country/Region').sum())\n",
    "country_confirmed_df = pd.DataFrame(country_confirmed_df.groupby('Country/Region').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Level Data ## \n",
    "- `us_state_confirmed_df ` - is the time series data for confirmed cases on a state level.\n",
    "- `us_state_deaths_df ` - is the time series data for deaths on a state level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_confirmed_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\")\n",
    "us_deaths_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\")\n",
    "us_states_confirmed_df = pd.DataFrame(us_confirmed_df.groupby('Province_State').sum())\n",
    "us_states_deaths_df = pd.DataFrame(us_deaths_df.groupby('Province_State').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Long/Lat Data for Countries and each States##\n",
    "We are going to be using Paul Mooneys Latitude and Longitude for Every Country and State data set from Kaggle - it is available [here](https://www.kaggle.com/paultimothymooney/latitude-and-longitude-for-every-country-and-stateworld_country_and_usa_states_latitude_and_longitude_values.csv)\n",
    "* ``` country_long_lat_df ``` stores all of the country level data\n",
    "* ``` usa_long_lat_df ``` stores the state level data for the United States \n",
    "\n",
    "And add this to our original data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_long_lat_df = pd.read_csv('data/world_country_and_usa_states_latitude_and_longitude_values.csv')[['country_code', 'latitude', 'longitude', 'country']]\n",
    "usa_long_lat_df = pd.read_csv('data/world_country_and_usa_states_latitude_and_longitude_values.csv')[['usa_state_code',\n",
    "       'usa_state_latitude', 'usa_state_longitude', 'usa_state']]\n",
    "country_confirmed_df = country_confirmed_df.merge(country_long_lat_df, right_on = 'country', left_on = 'Country/Region')\n",
    "country_deaths_df = country_deaths_df.merge(country_long_lat_df, right_on = 'country', left_on = 'Country/Region')\n",
    "\n",
    "us_states_confirmed_df = us_states_confirmed_df.merge(usa_long_lat_df, right_on = 'usa_state', left_on = 'Province_State')\n",
    "us_states_deaths_df = us_states_deaths_df.merge(usa_long_lat_df, right_on = 'usa_state', left_on = 'Province_State')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates/ Getting Data for Time Series Analysis ## \n",
    "We need to know what dates we are dealing with - so lets generate a list of the available dates in our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['UID', 'code3', 'FIPS', 'Lat', 'Long_', 'Population', '1/22/20',\n       '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20', '1/28/20',\n       '1/29/20', '1/30/20', '1/31/20', '2/1/20', '2/2/20', '2/3/20', '2/4/20',\n       '2/5/20', '2/6/20', '2/7/20', '2/8/20', '2/9/20', '2/10/20', '2/11/20',\n       '2/12/20', '2/13/20', '2/14/20', '2/15/20', '2/16/20', '2/17/20',\n       '2/18/20', '2/19/20', '2/20/20', '2/21/20', '2/22/20', '2/23/20',\n       '2/24/20', '2/25/20', '2/26/20', '2/27/20', '2/28/20', '2/29/20',\n       '3/1/20', '3/2/20', '3/3/20', '3/4/20', '3/5/20', '3/6/20', '3/7/20',\n       '3/8/20', '3/9/20', '3/10/20', '3/11/20', '3/12/20', '3/13/20',\n       '3/14/20', '3/15/20', '3/16/20', '3/17/20', '3/18/20', '3/19/20',\n       '3/20/20', '3/21/20', '3/22/20', '3/23/20', '3/24/20', '3/25/20',\n       '3/26/20', '3/27/20', '3/28/20', '3/29/20', '3/30/20', '3/31/20',\n       '4/1/20', 'usa_state_code', 'usa_state_latitude', 'usa_state_longitude',\n       'usa_state'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "\n",
    "rel_dates = list(country_confirmed_df.columns[country_confirmed_df.columns.str.contains(r\"\\d{1,2}\\/\\d{1,2}\\/\\d{1,2}\")])\n",
    "\n",
    "def stripAndFlip(df, index = 'country', date_range = rel_dates):\n",
    "    temp_df = df.copy()\n",
    "    temp_df.set_index(index)\n",
    "    temp_df = temp_df[date_range]\n",
    "    return temp_df\n",
    "\n",
    "ts_global_confirmed = stripAndFlip(country_confirmed_df)\n",
    "ts_global_deaths = stripAndFlip(country_deaths_df)\n",
    "ts_us_confirmed = stripAndFlip(us_states_confirmed_df, index = 'usa_state')\n",
    "ts_us_deaths = stripAndFlip(us_states_deaths_df, index = 'usa_state')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines ##\n",
    "Not doing anythign with it yet but lets set up some pipelines to make our life down the road\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'numerical_transformer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-00319ba11f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     12\u001b[0m dataclean = ColumnTransformer(transformers=[\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numerical_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "cat_trans = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy='constant')), #Subject to change please\n",
    "    ('ordinal', OrdinalEncoder())])\n",
    "num_trans = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy='constant'))\n",
    "])\n",
    "dataclean = ColumnTransformer(transformers=[\n",
    "        ('num', num_trans, numerical_cols),\n",
    "        ('cat', cat_trans, categorical_cols)\n",
    "    ])\n",
    "model = LinearRegression()\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', dataclean),\n",
    "                              ('model', model)\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}